<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>无深~博客</title>
  
  <subtitle>明天更好</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2024-12-18T03:11:58.522Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>无深</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>测试</title>
    <link href="http://example.com/posts/9daba997.html"/>
    <id>http://example.com/posts/9daba997.html</id>
    <published>2024-12-18T03:11:58.000Z</published>
    <updated>2024-12-18T03:11:58.522Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>No-Deep-GomokuView-AI项目介绍</title>
    <link href="http://example.com/posts/3.html"/>
    <id>http://example.com/posts/3.html</id>
    <published>2024-12-17T16:58:32.379Z</published>
    <updated>2024-12-17T17:04:27.630Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、项目概述"><a href="#一、项目概述" class="headerlink" title="一、项目概述"></a>一、项目概述</h2><p>本项目旨在使用飞桨框架 2.0 实现 AlphaZero 算法，构建一个能够玩五子棋的 AI 模型。通过纯粹的自我博弈方式进行训练，使 AI 在短时间内达到一定的棋力水平，能够与人类玩家进行有挑战性的对弈。</p><h2 id="二、五子棋游戏简介"><a href="#二、五子棋游戏简介" class="headerlink" title="二、五子棋游戏简介"></a>二、五子棋游戏简介</h2><p>五子棋是一款经典的两人对弈棋类游戏，双方分别使用黑白棋子，在棋盘竖线与横线的交叉点上轮流落子，率先形成五子连线的一方获胜。其规则简单易懂，上手容易，适合各个年龄段的人群，具有很高的趣味性和竞技性。</p><h2 id="三、本项目简介"><a href="#三、本项目简介" class="headerlink" title="三、本项目简介"></a>三、本项目简介</h2><p>本项目专注于运用 AlphaZero 算法来实现五子棋 AI。相较于复杂的围棋和象棋，五子棋的规则较为简洁，这使得我们能够将更多精力放在 AlphaZero 算法的训练和优化上。通过在一台普通 PC 机上进行几个小时的训练，即可获得一个具有一定实力的 AI 模型，在与人类玩家的对弈中展现出较强的竞争力。</p><h2 id="四、为什么使用-MCTS（蒙特卡洛树搜索）"><a href="#四、为什么使用-MCTS（蒙特卡洛树搜索）" class="headerlink" title="四、为什么使用 MCTS（蒙特卡洛树搜索）"></a>四、为什么使用 MCTS（蒙特卡洛树搜索）</h2><p>在传统的棋盘游戏决策过程中，玩家通常会思考多种走法及其可能的后续局面。类似 Minimax 这样的传统 AI 博弈树搜索算法，在做出决策前需要穷举所有可能的走法，这在面对复杂游戏时，其搜索空间会呈指数级增长，导致效率极其低下。例如，国际象棋的平均分支因子为 35，仅走两步就有 1,225（35²）种可能的棋面；围棋的平均分支因子更是高达 250，走两步就会产生 62,500（250²）种棋面。</p><p>而随着神经网络技术的发展，我们可以利用神经网络来指导搜索过程，筛选出更有价值的博弈路径进行探索，避免陷入大量无用的搜索分支中。蒙特卡洛树搜索（MCTS）算法在此背景下应运而生，它通过巧妙地结合神经网络的预测能力和树搜索的探索机制，有效地提高了搜索效率和决策质量。</p><h2 id="五、训练算法流程"><a href="#五、训练算法流程" class="headerlink" title="五、训练算法流程"></a>五、训练算法流程</h2><p>AlphaZero 算法的核心在于通过自我对弈不断收集数据，进而更新策略价值网络，而更新后的网络又会用于后续的自我对弈，形成一个相互促进、不断迭代的学习过程，从而实现 AI 棋力的稳步提升。</p><p>在本项目中，我们将训练流程封装在 <code>TrainPipeline</code> 类中，其中 <code>run</code> 方法是训练的主要执行逻辑。它会循环调用 <code>collect_selfplay_data</code> 方法来收集自我对弈产生的数据，当收集的数据量超过设定的 <code>batch_size</code> 时，就会调用 <code>policy_update</code> 方法对策略价值网络进行更新。在训练过程中，还可以根据实际需求调整各种超参数，如学习率、模拟次数、经验池大小等，以优化训练效果。</p><p>以下是 <code>TrainPipeline</code> 类的详细代码解释：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TrainPipeline</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, init_model=<span class="literal">None</span>, is_shown=<span class="number">0</span></span>):</span><br><span class="line">        <span class="comment"># 五子棋逻辑和棋盘UI的参数</span></span><br><span class="line">        <span class="variable language_">self</span>.board_width = <span class="number">9</span></span><br><span class="line">        <span class="variable language_">self</span>.board_height = <span class="number">9</span></span><br><span class="line">        <span class="variable language_">self</span>.n_in_row = <span class="number">5</span></span><br><span class="line">        <span class="variable language_">self</span>.board = Board(width=<span class="variable language_">self</span>.board_width,</span><br><span class="line">                           height=<span class="variable language_">self</span>.board_height,</span><br><span class="line">                           n_in_row=<span class="variable language_">self</span>.n_in_row)</span><br><span class="line">        <span class="variable language_">self</span>.is_shown = is_shown</span><br><span class="line">        <span class="variable language_">self</span>.game = Game_UI(<span class="variable language_">self</span>.board, is_shown)</span><br><span class="line">        <span class="comment"># 训练参数</span></span><br><span class="line">        <span class="variable language_">self</span>.learn_rate = <span class="number">2e-3</span></span><br><span class="line">        <span class="variable language_">self</span>.lr_multiplier = <span class="number">1.0</span>  <span class="comment"># 基于KL自适应地调整学习率</span></span><br><span class="line">        <span class="variable language_">self</span>.temp = <span class="number">1.0</span>  <span class="comment"># 临时变量</span></span><br><span class="line">        <span class="variable language_">self</span>.n_playout = <span class="number">400</span>  <span class="comment"># 每次移动的模拟次数</span></span><br><span class="line">        <span class="variable language_">self</span>.c_puct = <span class="number">5</span></span><br><span class="line">        <span class="variable language_">self</span>.buffer_size = <span class="number">10000</span>  <span class="comment"># 经验池大小 10000</span></span><br><span class="line">        <span class="variable language_">self</span>.batch_size = <span class="number">512</span>  <span class="comment"># 训练的mini-batch大小 512</span></span><br><span class="line">        <span class="variable language_">self</span>.data_buffer = deque(maxlen=<span class="variable language_">self</span>.buffer_size)</span><br><span class="line">        <span class="variable language_">self</span>.play_batch_size = <span class="number">1</span></span><br><span class="line">        <span class="variable language_">self</span>.epochs = <span class="number">5</span>  <span class="comment"># 每次更新的train_steps数量</span></span><br><span class="line">        <span class="variable language_">self</span>.kl_targ = <span class="number">0.02</span></span><br><span class="line">        <span class="variable language_">self</span>.check_freq = <span class="number">100</span>  <span class="comment"># 评估模型的频率，可以设置大一些比如500</span></span><br><span class="line">        <span class="variable language_">self</span>.game_batch_num = <span class="number">1500</span></span><br><span class="line">        <span class="variable language_">self</span>.best_win_ratio = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># 用于纯粹的mcts的模拟数量，用作评估训练策略的对手</span></span><br><span class="line">        <span class="variable language_">self</span>.pure_mcts_playout_num = <span class="number">1000</span></span><br><span class="line">        <span class="keyword">if</span> init_model:</span><br><span class="line">            <span class="comment"># 从初始的策略价值网开始训练</span></span><br><span class="line">            <span class="variable language_">self</span>.policy_value_net = PolicyValueNet(<span class="variable language_">self</span>.board_width,</span><br><span class="line">                                                   <span class="variable language_">self</span>.board_height,</span><br><span class="line">                                                   model_file=init_model)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 从新的策略价值网络开始训练</span></span><br><span class="line">            <span class="variable language_">self</span>.policy_value_net = PolicyValueNet(<span class="variable language_">self</span>.board_width,</span><br><span class="line">                                                   <span class="variable language_">self</span>.board_height)</span><br><span class="line">        <span class="comment"># 定义训练机器人</span></span><br><span class="line">        <span class="variable language_">self</span>.mcts_player = MCTSPlayer(<span class="variable language_">self</span>.policy_value_net.policy_value_fn,</span><br><span class="line">                                      c_puct=<span class="variable language_">self</span>.c_puct,</span><br><span class="line">                                      n_playout=<span class="variable language_">self</span>.n_playout,</span><br><span class="line">                                      is_selfplay=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li>在 <code>__init__</code> 方法中，首先初始化了与五子棋游戏逻辑和棋盘显示相关的参数，包括棋盘的宽度、高度、连成五子获胜的条件，以及创建了棋盘和游戏界面的实例。</li><li>接着定义了一系列训练参数，如学习率、学习率乘数（用于基于 KL 散度自适应调整学习率）、温度参数（用于控制探索程度）、每次移动的模拟次数、PUCT 算法中的 <code>c_puct</code> 参数、经验池大小、训练的小批次大小、训练轮数、KL 散度目标值、模型评估频率、总的游戏批次数量以及当前最佳胜率等。</li><li>根据是否提供初始模型文件路径，选择加载已有模型或创建新的策略价值网络实例，并创建基于该网络的 MCTS 玩家实例，用于自我对弈训练。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_equi_data</span>(<span class="params">self, play_data</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;通过旋转和翻转来增加数据集</span></span><br><span class="line"><span class="string">    play_data: [(state, mcts_prob, winner_z),...,...]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    extend_data = []</span><br><span class="line">    <span class="keyword">for</span> state, mcts_porb, winner <span class="keyword">in</span> play_data:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]:</span><br><span class="line">            <span class="comment"># 逆时针旋转</span></span><br><span class="line">            equi_state = np.array([np.rot90(s, i) <span class="keyword">for</span> s <span class="keyword">in</span> state])</span><br><span class="line">            equi_mcts_prob = np.rot90(np.flipud(</span><br><span class="line">                mcts_porb.reshape(<span class="variable language_">self</span>.board_height, <span class="variable language_">self</span>.board_width)), i)</span><br><span class="line">            extend_data.append((equi_state,</span><br><span class="line">                                np.flipud(equi_mcts_prob).flatten(),</span><br><span class="line">                                winner))</span><br><span class="line">            <span class="comment"># 水平翻转</span></span><br><span class="line">            equi_state = np.array([np.fliplr(s) <span class="keyword">for</span> s <span class="keyword">in</span> equi_state])</span><br><span class="line">            equi_mcts_prob = np.fliplr(equi_mcts_prob)</span><br><span class="line">            extend_data.append((equi_state,</span><br><span class="line">                                np.flipud(equi_mcts_prob).flatten(),</span><br><span class="line">                                winner))</span><br><span class="line">    <span class="keyword">return</span> extend_data</span><br></pre></td></tr></table></figure><ul><li><code>get_equi_data</code> 方法用于数据增强，通过对原始的自我对弈数据进行旋转和翻转操作，扩充数据集的多样性，从而提高模型的泛化能力。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">collect_selfplay_data</span>(<span class="params">self, n_games=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;收集自我博弈数据进行训练&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_games):</span><br><span class="line">        winner, play_data = <span class="variable language_">self</span>.game.start_self_play(<span class="variable language_">self</span>.mcts_player, temp=<span class="variable language_">self</span>.temp)</span><br><span class="line">        play_data = <span class="built_in">list</span>(play_data)[:]</span><br><span class="line">        <span class="variable language_">self</span>.episode_len = <span class="built_in">len</span>(play_data)</span><br><span class="line">        <span class="comment"># 增加数据</span></span><br><span class="line">        play_data = <span class="variable language_">self</span>.get_equi_data(play_data)</span><br><span class="line">        <span class="variable language_">self</span>.data_buffer.extend(play_data)</span><br></pre></td></tr></table></figure><ul><li><code>collect_selfplay_data</code> 方法负责收集自我对弈数据。在每次自我对弈中，通过 <code>start_self_play</code> 方法进行一局游戏，获取获胜者和游戏过程中的数据（包括状态、MCTS 概率和获胜者信息），然后对这些数据进行扩充，并添加到数据缓冲区 <code>data_buffer</code> 中，为后续的训练提供数据支持。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">policy_update</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;更新策略价值网络&quot;&quot;&quot;</span></span><br><span class="line">    mini_batch = random.sample(<span class="variable language_">self</span>.data_buffer, <span class="variable language_">self</span>.batch_size)</span><br><span class="line">    state_batch = [data[<span class="number">0</span>] <span class="keyword">for</span> data <span class="keyword">in</span> mini_batch]</span><br><span class="line">    state_batch = np.array(state_batch).astype(<span class="string">&quot;float32&quot;</span>)</span><br><span class="line">    mcts_probs_batch = [data[<span class="number">1</span>] <span class="keyword">for</span> data <span class="keyword">in</span> mini_batch]</span><br><span class="line">    mcts_probs_batch = np.array(mcts_probs_batch).astype(<span class="string">&quot;float32&quot;</span>)</span><br><span class="line">    winner_batch = [data[<span class="number">2</span>] <span class="keyword">for</span> data <span class="keyword">in</span> mini_batch]</span><br><span class="line">    winner_batch = np.array(winner_batch).astype(<span class="string">&quot;float32&quot;</span>)</span><br><span class="line">    old_probs, old_v = <span class="variable language_">self</span>.policy_value_net.policy_value(state_batch)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.epochs):</span><br><span class="line">        loss, entropy = <span class="variable language_">self</span>.policy_value_net.train_step(</span><br><span class="line">            state_batch,</span><br><span class="line">            mcts_probs_batch,</span><br><span class="line">            winner_batch,</span><br><span class="line">            <span class="variable language_">self</span>.learn_rate * <span class="variable language_">self</span>.lr_multiplier)</span><br><span class="line">        new_probs, new_v = <span class="variable language_">self</span>.policy_value_net.policy_value(state_batch)</span><br><span class="line">        kl = np.mean(np.<span class="built_in">sum</span>(old_probs * (</span><br><span class="line">            np.log(old_probs + <span class="number">1e-10</span>) - np.log(new_probs + <span class="number">1e-10</span>)),</span><br><span class="line">                            axis=<span class="number">1</span>)</span><br><span class="line">                     )</span><br><span class="line">        <span class="keyword">if</span> kl &gt; <span class="variable language_">self</span>.kl_targ * <span class="number">4</span>:  <span class="comment"># early stopping if D_KL diverges badly</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># 自适应调节学习率</span></span><br><span class="line">    <span class="keyword">if</span> kl &gt; <span class="variable language_">self</span>.kl_targ * <span class="number">2</span> <span class="keyword">and</span> <span class="variable language_">self</span>.lr_multiplier &gt; <span class="number">0.1</span>:</span><br><span class="line">        <span class="variable language_">self</span>.lr_multiplier /= <span class="number">1.5</span></span><br><span class="line">    <span class="keyword">elif</span> kl &lt; <span class="variable language_">self</span>.kl_targ / <span class="number">2</span> <span class="keyword">and</span> <span class="variable language_">self</span>.lr_multiplier &lt; <span class="number">10</span>:</span><br><span class="line">        <span class="variable language_">self</span>.lr_multiplier *= <span class="number">1.5</span></span><br><span class="line"></span><br><span class="line">    explained_var_old = (<span class="number">1</span> -</span><br><span class="line">                         np.var(np.array(winner_batch) - old_v.flatten()) /</span><br><span class="line">                         np.var(np.array(winner_batch)))</span><br><span class="line">    explained_var_new = (<span class="number">1</span> -</span><br><span class="line">                         np.var(np.array(winner_batch) - new_v.flatten()) /</span><br><span class="line">                         np.var(np.array(winner_batch)))</span><br><span class="line">    <span class="built_in">print</span>((<span class="string">&quot;kl:&#123;:.5f&#125;,&quot;</span></span><br><span class="line">           <span class="string">&quot;lr_multiplier:&#123;:.3f&#125;,&quot;</span></span><br><span class="line">           <span class="string">&quot;loss:&#123;&#125;,&quot;</span></span><br><span class="line">           <span class="string">&quot;entropy:&#123;&#125;,&quot;</span></span><br><span class="line">           <span class="string">&quot;explained_var_old:&#123;:.3f&#125;,&quot;</span></span><br><span class="line">           <span class="string">&quot;explained_var_new:&#123;:.3f&#125;&quot;</span></span><br><span class="line">           ).<span class="built_in">format</span>(kl,</span><br><span class="line">                    <span class="variable language_">self</span>.lr_multiplier,</span><br><span class="line">                    loss,</span><br><span class="line">                    entropy,</span><br><span class="line">                    explained_var_old,</span><br><span class="line">                    explained_var_new))</span><br><span class="line">    <span class="keyword">return</span> loss, entropy</span><br></pre></td></tr></table></figure><ul><li><code>policy_update</code> 方法用于更新策略价值网络。首先从数据缓冲区中随机采样一个小批次的数据，包括状态、MCTS 概率和获胜者信息，并将其转换为适合网络输入的格式。然后通过多次调用 <code>train_step</code> 方法进行训练，计算价值损失和策略损失，并进行反向传播和参数优化。在训练过程中，还会计算新旧策略的 KL 散度，根据 KL 散度的值自适应地调整学习率乘数，同时计算并打印出 KL 散度、学习率乘数、损失、熵以及解释方差等训练信息，最后返回损失和熵的值。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">policy_evaluate</span>(<span class="params">self, n_games=<span class="number">10</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    通过与纯的MCTS算法对抗来评估训练的策略</span></span><br><span class="line"><span class="string">    注意：这仅用于监控训练进度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    current_mcts_player = MCTSPlayer(<span class="variable language_">self</span>.policy_value_net.policy_value_fn,</span><br><span class="line">                                     c_puct=<span class="variable language_">self</span>.c_puct,</span><br><span class="line">                                     n_playout=<span class="variable language_">self</span>.n_playout)</span><br><span class="line">    pure_mcts_player = MCTS_Pure(c_puct=<span class="number">5</span>,</span><br><span class="line">                                 n_playout=<span class="variable language_">self</span>.pure_mcts_playout_num)</span><br><span class="line">    win_cnt = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_games):</span><br><span class="line">        winner = <span class="variable language_">self</span>.game.start_play(current_mcts_player,</span><br><span class="line">                                      pure_mcts_player,</span><br><span class="line">                                      start_player=i % <span class="number">2</span>)</span><br><span class="line">        win_cnt[winner] += <span class="number">1</span></span><br><span class="line">    win_ratio = <span class="number">1.0</span> * (win_cnt[<span class="number">1</span>] + <span class="number">0.5</span> * win_cnt[-<span class="number">1</span>]) / n_games</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;num_playouts:&#123;&#125;, win: &#123;&#125;, lose: &#123;&#125;, tie:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">        <span class="variable language_">self</span>.pure_mcts_playout_num,</span><br><span class="line">        win_cnt[<span class="number">1</span>], win_cnt[<span class="number">2</span>], win_cnt[-<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> win_ratio</span><br></pre></td></tr></table></figure><ul><li><code>policy_evaluate</code> 方法用于评估训练的策略。通过创建基于当前策略价值网络的 MCTS 玩家和一个纯 MCTS 玩家进行对弈，统计在一定数量的游戏中双方的胜负情况，计算出当前策略的胜率，并打印相关信息，返回胜率值。该方法主要用于监控训练过程中策略的性能提升情况。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;开始训练&quot;&quot;&quot;</span></span><br><span class="line">    root = os.getcwd()</span><br><span class="line">    dst_path = os.path.join(root, <span class="string">&#x27;dist&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(dst_path):</span><br><span class="line">        os.makedirs(dst_path)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.game_batch_num):</span><br><span class="line">            <span class="variable language_">self</span>.collect_selfplay_data(<span class="variable language_">self</span>.play_batch_size)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;batch i:&#123;&#125;, episode_len:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                i + <span class="number">1</span>, <span class="variable language_">self</span>.episode_len))</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data_buffer) &gt; <span class="variable language_">self</span>.batch_size:</span><br><span class="line">                loss, entropy = <span class="variable language_">self</span>.policy_update()</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;loss :&#123;&#125;, entropy:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(loss, entropy))</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="variable language_">self</span>.policy_value_net.save_model(os.path.join(dst_path, <span class="string">&#x27;current_policy_step.model&#x27;</span>))</span><br><span class="line">            <span class="comment"># 检查当前模型的性能，保存模型的参数</span></span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="variable language_">self</span>.check_freq == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;current self-play batch: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i + <span class="number">1</span>))</span><br><span class="line">                win_ratio = <span class="variable language_">self</span>.policy_evaluate()</span><br><span class="line">                <span class="variable language_">self</span>.policy_value_net.save_model(os.path.join(dst_path, <span class="string">&#x27;current_policy.model&#x27;</span>))</span><br><span class="line">                <span class="keyword">if</span> win_ratio &gt; <span class="variable language_">self</span>.best_win_ratio:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;New best policy!!!!!!!!&quot;</span>)</span><br><span class="line">                    <span class="variable language_">self</span>.best_win_ratio = win_ratio</span><br><span class="line">                    <span class="comment"># 更新最好的策略</span></span><br><span class="line">                    <span class="variable language_">self</span>.policy_value_net.save_model(os.path.join(dst_path, <span class="string">&#x27;best_policy.model&#x27;</span>))</span><br><span class="line">                    <span class="keyword">if</span> (<span class="variable language_">self</span>.best_win_ratio == <span class="number">1.0</span> <span class="keyword">and</span></span><br><span class="line">                                <span class="variable language_">self</span>.pure_mcts_playout_num &lt; <span class="number">8000</span>):</span><br><span class="line">                        <span class="variable language_">self</span>.pure_mcts_playout_num += <span class="number">1000</span></span><br><span class="line">                        <span class="variable language_">self</span>.best_win_ratio = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">except</span> KeyboardInterrupt:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\n\rquit&#x27;</span>)</span><br></pre></td></tr></table></figure><ul><li><code>run</code> 方法是训练的主循环逻辑。首先创建保存模型的目录，如果不存在则自动创建。然后在循环中，不断地收集自我对弈数据，当数据量足够时进行策略更新，并定期保存当前的模型参数。每隔一定的批次数量，还会对当前模型进行性能评估，根据评估结果保存最佳模型参数，并根据胜率情况调整用于评估的纯 MCTS 玩家的模拟次数，以适应模型的不断进化。如果在训练过程中遇到键盘中断，则会打印退出信息并停止训练。</li></ul><p>在 <code>if __name__ == &#39;__main__&#39;:</code> 部分，首先获取当前的计算设备（CPU 或 GPU），并设置飞桨的计算设备。然后根据指定的模型路径（如果有）创建 <code>TrainPipeline</code> 实例，并调用 <code>run</code> 方法开始训练过程。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、项目概述&quot;&gt;&lt;a href=&quot;#一、项目概述&quot; class=&quot;headerlink&quot; title=&quot;一、项目概述&quot;&gt;&lt;/a&gt;一、项目概述&lt;/h2&gt;&lt;p&gt;本项目旨在使用飞桨框架 2.0 实现 AlphaZero 算法，构建一个能够玩五子棋的 AI 模型。通过纯粹</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>队伍</title>
    <link href="http://example.com/posts/2.html"/>
    <id>http://example.com/posts/2.html</id>
    <published>2024-12-11T15:23:38.948Z</published>
    <updated>2024-12-11T15:24:09.861Z</updated>
    
    <content type="html"><![CDATA[<h1 id="团队成员简介"><a href="#团队成员简介" class="headerlink" title="团队成员简介"></a>团队成员简介</h1><h2 id="刘来来"><a href="#刘来来" class="headerlink" title="刘来来"></a>刘来来</h2><ul><li><strong>学号</strong>: 20224811139</li><li><strong>简介</strong>: 刘来来，团队中的积极分子，擅长数据分析和项目管理。在团队中担任协调和规划的角色，确保项目的顺利进行。</li></ul><h2 id="张莉"><a href="#张莉" class="headerlink" title="张莉"></a>张莉</h2><ul><li><strong>学号</strong>: 20224811159</li><li><strong>简介</strong>: 张莉，团队的技术骨干，精通多种编程语言和数据处理工具。在团队中负责技术开发和问题解决，是团队技术进步的推动者。</li></ul><h2 id="谢丹花"><a href="#谢丹花" class="headerlink" title="谢丹花"></a>谢丹花</h2><ul><li><strong>学号</strong>: 20224811143</li><li><strong>简介</strong>: 谢丹花，团队的创意源泉，具有出色的创意思维和设计能力。在团队中负责创意设计和视觉呈现，为团队的作品增添独特的风格和魅力。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;团队成员简介&quot;&gt;&lt;a href=&quot;#团队成员简介&quot; class=&quot;headerlink&quot; title=&quot;团队成员简介&quot;&gt;&lt;/a&gt;团队成员简介&lt;/h1&gt;&lt;h2 id=&quot;刘来来&quot;&gt;&lt;a href=&quot;#刘来来&quot; class=&quot;headerlink&quot; title=&quot;刘来来</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>一策惠三区·脱贫攻坚的共鸣</title>
    <link href="http://example.com/posts/1.html"/>
    <id>http://example.com/posts/1.html</id>
    <published>2024-12-11T15:15:11.682Z</published>
    <updated>2024-12-11T15:19:43.401Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p>本作品基于汇总全国脱贫人口数量、贵州省脱贫攻坚情况以及脱贫攻坚殉职人员名单等，数据集记录了贵州省在脱贫攻坚战役中英勇献身每一位英雄的事迹，由贵州省相关政府部门公开发布，确保了信息的真实性和权威性。</p><h3 id="图1-数据部分截图"><a href="#图1-数据部分截图" class="headerlink" title="图1 数据部分截图"></a>图1 数据部分截图</h3><h2 id="数据规模"><a href="#数据规模" class="headerlink" title="数据规模"></a>数据规模</h2><p>数据集共收录了12位殉职英雄的事迹，时间跨度从2015年至2021年，覆盖了贵州省多个地区的脱贫攻坚前线。</p><h2 id="技术路线"><a href="#技术路线" class="headerlink" title="技术路线"></a>技术路线</h2><p>本次作品我们采用现代数据处理与可视化技术，包括可视化设计、交互界面开发（运用HTML和JavaScript构建了一个用户更好查询的网页）允许各用户通过网页点击、筛选等方式探索贵州省脱贫攻坚的相关事迹。</p><h2 id="创意思路"><a href="#创意思路" class="headerlink" title="创意思路"></a>创意思路</h2><p>作品题目“一策惠三区·脱贫攻坚的共鸣”，深刻揭示了国家层面脱贫攻坚政策的深远影响。“一策”即指国家层面的脱贫攻坚战略，而“三区”则特指西藏——西藏自治区；新疆南疆四地州——包括和田、阿克苏、喀什、克孜勒苏柯尔克孜自治州；以及青海、甘肃、四川和云南四省涉藏地区。这些区域构成了国家全面建成小康社会中最为艰巨的挑战，是深度贫困的核心地带，因此受到了国家层面的特别关注与大力支持。本作品旨在通过数据可视化的手法，向那些为脱贫攻坚事业默默奉献的英雄们致敬，展现他们无私的奉献精神。</p><h3 id="（1）网页模块："><a href="#（1）网页模块：" class="headerlink" title="（1）网页模块："></a>（1）网页模块：</h3><h4 id="图2-模块一（星辉）"><a href="#图2-模块一（星辉）" class="headerlink" title="图2 模块一（星辉）"></a>图2 模块一（星辉）</h4><h4 id="图3-模块二（攻坚）"><a href="#图3-模块二（攻坚）" class="headerlink" title="图3 模块二（攻坚）"></a>图3 模块二（攻坚）</h4><h4 id="图4-模块三（星火）"><a href="#图4-模块三（星火）" class="headerlink" title="图4 模块三（星火）"></a>图4 模块三（星火）</h4><h3 id="（2）可视化展现："><a href="#（2）可视化展现：" class="headerlink" title="（2）可视化展现："></a>（2）可视化展现：</h3><h4 id="图5-部分省份贫困人口折线图"><a href="#图5-部分省份贫困人口折线图" class="headerlink" title="图5 部分省份贫困人口折线图"></a>图5 部分省份贫困人口折线图</h4><h4 id="图6-贫困人口饼图"><a href="#图6-贫困人口饼图" class="headerlink" title="图6 贫困人口饼图"></a>图6 贫困人口饼图</h4><p>作品不仅展示了贵州省在脱贫攻坚战中的辉煌成就，更重要的是，它通过一个个鲜活的人物故事，传达了扶贫攻坚路上的艰辛与希望。希望通过“一策惠三区：脱贫攻坚的共鸣”这一作品，向所有为脱贫攻坚作出贡献的人士致以最高的敬意，希望在脱贫攻坚的基础上，乡村振兴取得更好的成果。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;数据来源&quot;&gt;&lt;a href=&quot;#数据来源&quot; class=&quot;headerlink&quot; title=&quot;数据来源&quot;&gt;&lt;/a&gt;数据来源&lt;/h2&gt;&lt;p&gt;本作品基于汇总全国脱贫人口数量、贵州省脱贫攻坚情况以及脱贫攻坚殉职人员名单等，数据集记录了贵州省在脱贫攻坚战役中英勇献身每一</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>无深个人博客说明</title>
    <link href="http://example.com/posts/0.html"/>
    <id>http://example.com/posts/0.html</id>
    <published>2024-09-19T02:50:34.531Z</published>
    <updated>2024-09-19T16:11:15.552Z</updated>
    
    <content type="html"><![CDATA[<div class="tabs" id="test1"><ul class="nav-tabs"><button type="button" class="tab  active" data-href="test1-1">test1 1</button><button type="button" class="tab " data-href="test1-2">test1 2</button><button type="button" class="tab " data-href="test1-3">test1 3</button></ul><div class="tab-contents"><div class="tab-item-content active" id="test1-1"><p><strong>This is Tab one.</strong></p></div><div class="tab-item-content" id="test1-2"><p><strong>This is Tab 2.</strong></p></div><div class="tab-item-content" id="test1-3"><p><strong>This is Tab 3.</strong></p></div></div><div class="tab-to-top"><button type="button" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div>]]></content>
    
    
      
      
    <summary type="html">&lt;div class=&quot;tabs&quot; id=&quot;test1&quot;&gt;&lt;ul class=&quot;nav-tabs&quot;&gt;&lt;button type=&quot;button&quot; class=&quot;tab  active&quot; data-href=&quot;test1-1&quot;&gt;test1 1&lt;/button&gt;&lt;button type</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>无深获奖墙</title>
    <link href="http://example.com/posts/4a17b156.html"/>
    <id>http://example.com/posts/4a17b156.html</id>
    <published>2024-09-14T07:27:34.997Z</published>
    <updated>2024-12-11T15:26:27.346Z</updated>
    
    <content type="html"><![CDATA[<p>获奖墙</p><span class='p center logo large'>Wu Shen</span><div class="tabs" id="test1"><ul class="nav-tabs"><button type="button" class="tab  active" data-href="test1-1">test1 1</button><button type="button" class="tab " data-href="test1-2">test1 2</button><button type="button" class="tab " data-href="test1-3">test1 3</button></ul><div class="tab-contents"><div class="tab-item-content active" id="test1-1"><p><strong>This is Tab one.</strong></p></div><div class="tab-item-content" id="test1-2"><p><strong>This is Tab 2.</strong></p></div><div class="tab-item-content" id="test1-3"><p><strong>This is Tab 3.</strong></p></div></div><div class="tab-to-top"><button type="button" aria-label="scroll to top"><i class="fas fa-arrow-up"></i></button></div></div>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;获奖墙&lt;/p&gt;
&lt;span class=&#39;p center logo large&#39;&gt;Wu Shen&lt;/span&gt;



&lt;div class=&quot;tabs&quot; id=&quot;test1&quot;&gt;&lt;ul class=&quot;nav-tabs&quot;&gt;&lt;button type=&quot;button&quot; class</summary>
      
    
    
    
    
  </entry>
  
</feed>
